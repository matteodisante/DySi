{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis: OAT vs Variance-Based Methods\n",
    "\n",
    "This notebook demonstrates sensitivity analysis techniques for rocket simulations, comparing:\n",
    "\n",
    "1. **OAT (One-At-a-Time)**: Quick parameter screening method\n",
    "2. **Variance-Based**: Rigorous statistical method following RocketPy standards\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand when to use each sensitivity method\n",
    "- Integrate Monte Carlo simulations with sensitivity analysis\n",
    "- Interpret sensitivity coefficients and Linear Approximation Error (LAE)\n",
    "- Identify critical parameters for rocket design\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Run the Monte Carlo notebook (`02_monte_carlo_ensemble.ipynb`) first to generate simulation data."
   ]
  },
  {
   "cell_type": "code",
   "source": "from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom src.config_loader import ConfigLoader\nfrom src.monte_carlo_runner import MonteCarloRunner\nfrom src.variance_sensitivity import VarianceBasedSensitivityAnalyzer\nfrom src.sensitivity_analyzer import OATSensitivityAnalyzer\nfrom src.sensitivity_utils import (\n    estimate_parameter_statistics,\n    create_sensitivity_comparison,\n    filter_significant_parameters\n)\n\nplt.style.use('seaborn-v0_8-darkgrid')\n%matplotlib inline\n\nprint(\"✓ All imports successful\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Theory - Understanding Sensitivity Methods\n",
    "\n",
    "### OAT (One-At-a-Time) Method\n",
    "\n",
    "**How it works:**\n",
    "1. Run baseline simulation with nominal parameters\n",
    "2. For each parameter, vary it by ±X% while keeping others fixed\n",
    "3. Calculate sensitivity index from output changes\n",
    "\n",
    "**Pros:**\n",
    "- Simple and intuitive\n",
    "- Shows directional effects (increase/decrease)\n",
    "- Good for quick parameter screening\n",
    "\n",
    "**Cons:**\n",
    "- Doesn't quantify variance contribution\n",
    "- Expensive: requires 2N+1 simulations\n",
    "- No statistical validation\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Sensitivity Index = |((high - low) / 2 / baseline) / (variation_percent / 100)|\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Variance-Based Method (RocketPy Standard)\n",
    "\n",
    "**How it works:**\n",
    "1. Run Monte Carlo simulation with parameter uncertainties\n",
    "2. Fit multiple linear regression: y = β₀ + β₁x₁ + β₂x₂ + ...\n",
    "3. Calculate sensitivity coefficients from variance decomposition\n",
    "\n",
    "**Pros:**\n",
    "- Quantifies variance contribution (percentage)\n",
    "- Statistical rigor with confidence intervals\n",
    "- Efficient: reuses Monte Carlo data\n",
    "- Validated with Linear Approximation Error (LAE)\n",
    "\n",
    "**Cons:**\n",
    "- Requires Monte Carlo simulation first\n",
    "- Assumes linear relationships (check LAE)\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "S(j) = 100 × (β_j² × σ_j²) / (σ_ε² + Σ_k(σ_k² × β_k²))\n",
    "\n",
    "where:\n",
    "- S(j) = sensitivity coefficient for parameter j\n",
    "- β_j = regression coefficient\n",
    "- σ_j = standard deviation of parameter j\n",
    "- σ_ε = residual standard error\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- S(j) = 70% means: if parameter j were known perfectly, output variance would decrease by 70%\n",
    "- LAE < 10%: Linear approximation is excellent\n",
    "- LAE > 30%: Non-linear effects may be significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setup - Load Configuration and Run Monte Carlo\n",
    "\n",
    "First, we need Monte Carlo simulation data for variance-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = Path('../configs/complete_example.yaml')\n",
    "\n",
    "config_loader = ConfigLoader()\n",
    "config_loader.load_from_yaml(str(config_path))\n",
    "\n",
    "rocket_cfg = config_loader.get_rocket_config()\n",
    "motor_cfg = config_loader.get_motor_config()\n",
    "env_cfg = config_loader.get_environment_config()\n",
    "sim_cfg = config_loader.get_simulation_config()\n",
    "\n",
    "print(f\"Configuration loaded: {rocket_cfg.name}\")\n",
    "print(f\"Dry mass: {rocket_cfg.dry_mass_kg:.2f} kg\")\n",
    "print(f\"Motor: {motor_cfg.thrust_source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load configuration\nconfig_path = Path('../configs/monte_carlo/01_basic_mc.yaml')\n\nconfig_loader = ConfigLoader()\nconfig_loader.load_from_yaml(str(config_path))\n\nrocket_cfg = config_loader.get_rocket_config()\nmotor_cfg = config_loader.get_motor_config()\nenv_cfg = config_loader.get_environment_config()\nsim_cfg = config_loader.get_simulation_config()\n\nprint(f\"Configuration loaded: {rocket_cfg.name}\")\nprint(f\"Dry mass: {rocket_cfg.dry_mass_kg:.2f} kg\")\nprint(f\"Motor: {motor_cfg.thrust_source}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parameter variations\n",
    "# These represent measurement uncertainties and manufacturing tolerances\n",
    "\n",
    "mc_runner.add_parameter_variation(\n",
    "    parameter_path=\"rocket.dry_mass_kg\",\n",
    "    mean=rocket_cfg.dry_mass_kg,\n",
    "    std=0.5,  # ±0.5 kg uncertainty\n",
    "    distribution=\"normal\"\n",
    ")\n",
    "\n",
    "mc_runner.add_parameter_variation(\n",
    "    parameter_path=\"environment.wind.velocity_ms\",\n",
    "    mean=env_cfg.wind.velocity_ms,\n",
    "    std=2.0,  # ±2 m/s wind variation\n",
    "    distribution=\"normal\"\n",
    ")\n",
    "\n",
    "mc_runner.add_parameter_variation(\n",
    "    parameter_path=\"rocket.fins.root_chord_m\",\n",
    "    mean=rocket_cfg.fins.root_chord_m,\n",
    "    std=0.005,  # ±5mm manufacturing tolerance\n",
    "    distribution=\"normal\"\n",
    ")\n",
    "\n",
    "print(\"Parameter variations configured:\")\n",
    "for param, config in mc_runner.parameter_variations.items():\n",
    "    print(f\"  {param}: μ={config['mean']:.3f}, σ={config['std']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo simulation\n",
    "print(\"Running Monte Carlo simulation (this may take a few minutes)...\")\n",
    "\n",
    "results = mc_runner.run(parallel=True, max_workers=4)\n",
    "\n",
    "print(f\"\\nSimulation complete!\")\n",
    "print(f\"Successful: {len(results)}/{mc_runner.num_simulations}\")\n",
    "print(f\"Failed: {len(mc_runner.failed_simulations)}\")\n",
    "\n",
    "# Show statistics\n",
    "mc_runner.print_statistics_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Variance-Based Sensitivity Analysis\n",
    "\n",
    "Now we perform variance-based sensitivity analysis on the Monte Carlo results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Monte Carlo data for sensitivity analysis\n",
    "parameters_df, targets_df = mc_runner.export_for_sensitivity(\n",
    "    parameter_names=list(mc_runner.parameter_variations.keys()),\n",
    "    target_names=[\"apogee_m\", \"max_velocity_ms\", \"lateral_distance_m\"]\n",
    ")\n",
    "\n",
    "print(f\"Exported {len(parameters_df)} samples\")\n",
    "print(f\"Parameters: {list(parameters_df.columns)}\")\n",
    "print(f\"Targets: {list(targets_df.columns)}\")\n",
    "\n",
    "# Preview data\n",
    "print(\"\\nParameter samples (first 5):\")\n",
    "display(parameters_df.head())\n",
    "\n",
    "print(\"\\nTarget variables (first 5):\")\n",
    "display(targets_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variance-based sensitivity analyzer\n",
    "analyzer = VarianceBasedSensitivityAnalyzer(\n",
    "    parameter_names=list(parameters_df.columns),\n",
    "    target_names=list(targets_df.columns)\n",
    ")\n",
    "\n",
    "# Set nominal parameters from Monte Carlo configuration\n",
    "metadata = mc_runner.get_parameter_metadata()\n",
    "means = {param: meta['mean'] for param, meta in metadata.items()}\n",
    "stds = {param: meta['std'] for param, meta in metadata.items()}\n",
    "\n",
    "analyzer.set_nominal_parameters(means=means, stds=stds)\n",
    "\n",
    "print(\"Analyzer configured with nominal parameters:\")\n",
    "for param in parameters_df.columns:\n",
    "    print(f\"  {param}: μ={means[param]:.3f}, σ={stds[param]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression models\n",
    "print(\"Fitting regression models...\\n\")\n",
    "analyzer.fit(parameters_df, targets_df)\n",
    "\n",
    "print(\"✓ Regression models fitted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive sensitivity summary\n",
    "analyzer.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "The sensitivity table shows:\n",
    "\n",
    "1. **Sensitivity(%)**: Percentage contribution to output variance\n",
    "   - If you could measure this parameter perfectly (no uncertainty), how much would the output variance decrease?\n",
    "   - Example: S = 70% means 70% of output variance is due to this parameter's uncertainty\n",
    "\n",
    "2. **Nominal mean/sd**: The mean and standard deviation used in the analysis\n",
    "\n",
    "3. **Effect per sd**: Change in output for one standard deviation change in parameter\n",
    "   - Units: [output units] per [input standard deviation]\n",
    "   - Shows practical impact magnitude\n",
    "\n",
    "4. **Linear Approximation Error (LAE)**:\n",
    "   - LAE < 10%: Linear model is excellent ✓\n",
    "   - LAE < 30%: Linear model is adequate ~\n",
    "   - LAE > 30%: Non-linear effects significant ✗\n",
    "\n",
    "**Key Insight**: Focus on parameters with sensitivity > LAE. Parameters with sensitivity < LAE may not be statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get importance ranking for apogee\n",
    "ranking_apogee = analyzer.get_importance_ranking(\"apogee_m\")\n",
    "\n",
    "print(\"Parameter Importance Ranking for Apogee:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (param, sensitivity) in enumerate(ranking_apogee, 1):\n",
    "    print(f\"{i}. {param:<35} {sensitivity:>6.2f}%\")\n",
    "\n",
    "# Identify significant parameters\n",
    "lae_apogee = analyzer.lae[\"apogee_m\"]\n",
    "sensitivities = analyzer.sensitivity_coefficients[\"apogee_m\"]\n",
    "\n",
    "significant_params = filter_significant_parameters(sensitivities, lae_apogee)\n",
    "\n",
    "print(f\"\\nSignificant parameters (sensitivity > LAE={lae_apogee:.2f}%):\")\n",
    "for param in significant_params:\n",
    "    print(f\"  ✓ {param}: {sensitivities[param]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sensitivity bar plots for all targets\n",
    "output_dir = Path('../outputs/sensitivity')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for target_name in targets_df.columns:\n",
    "    plot_path = output_dir / f\"sensitivity_bars_{target_name}.png\"\n",
    "    \n",
    "    fig = analyzer.plot_sensitivity_bars(\n",
    "        output_path=str(plot_path),\n",
    "        target_name=target_name,\n",
    "        figsize=(10, 6)\n",
    "    )\n",
    "    \n",
    "    print(f\"Saved plot: {plot_path}\")\n",
    "    \n",
    "    # Display inline\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: OAT Sensitivity Analysis (for Comparison)\n",
    "\n",
    "Now let's run OAT analysis on the same parameters to compare methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OAT analyzer\n",
    "oat_analyzer = OATSensitivityAnalyzer(\n",
    "    base_rocket_config=rocket_cfg,\n",
    "    base_motor_config=motor_cfg,\n",
    "    base_environment_config=env_cfg,\n",
    "    base_simulation_config=sim_cfg\n",
    ")\n",
    "\n",
    "# Add same parameters with 10% variation\n",
    "for param in parameters_df.columns:\n",
    "    oat_analyzer.add_parameter(\n",
    "        parameter_path=param,\n",
    "        variation_percent=10.0\n",
    "    )\n",
    "\n",
    "print(\"OAT analyzer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OAT analysis for apogee\n",
    "print(\"Running OAT sensitivity analysis...\\n\")\n",
    "oat_analyzer.run(output_metric=\"apogee_m\")\n",
    "\n",
    "# Print summary\n",
    "oat_analyzer.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate OAT tornado diagram\n",
    "tornado_path = output_dir / \"tornado_diagram_oat.png\"\n",
    "\n",
    "oat_analyzer.plot_tornado_diagram(\n",
    "    output_path=str(tornado_path),\n",
    "    title=\"OAT Sensitivity: Apogee (±10% variation)\"\n",
    ")\n",
    "\n",
    "print(f\"Saved tornado diagram: {tornado_path}\")\n",
    "\n",
    "# Display\n",
    "img = plt.imread(tornado_path)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comparing OAT vs Variance-Based Methods\n",
    "\n",
    "Let's compare the results from both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OAT ranking\n",
    "oat_ranking = oat_analyzer.get_importance_ranking()\n",
    "\n",
    "print(\"Method Comparison for Apogee\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Parameter':<35} {'Variance-Based':<20} {'OAT Index':<15}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for param in parameters_df.columns:\n",
    "    var_sens = sensitivities.get(param, 0)\n",
    "    oat_sens = oat_analyzer.sensitivity_results.get(param, {}).get('sensitivity_index', 0)\n",
    "    \n",
    "    print(f\"{param:<35} {var_sens:>8.2f}% {' '*10} {oat_sens:>8.3f}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"LAE: {lae_apogee:.2f}%\")\n",
    "print(\"\\nNote: Variance-based values are percentages of variance contribution.\")\n",
    "print(\"OAT values are dimensionless sensitivity indices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences\n",
    "\n",
    "| Aspect | Variance-Based | OAT |\n",
    "|--------|---------------|-----|\n",
    "| **Metric** | Variance contribution (%) | Sensitivity index (dimensionless) |\n",
    "| **Interpretation** | \"How much variance\" | \"How sensitive\" |\n",
    "| **Simulations** | Reuses MC data (efficient) | 2N+1 new runs |\n",
    "| **Statistical rigor** | Yes (LAE validation) | No |\n",
    "| **Ranking** | Usually similar | Usually similar |\n",
    "| **Best for** | Quantitative analysis, publications | Quick screening |\n",
    "\n",
    "**When rankings differ:** Check LAE. High LAE suggests non-linear effects that OAT may capture better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Practical Insights - Design Recommendations\n",
    "\n",
    "Based on sensitivity analysis, we can make design recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate potential variance reduction\n",
    "from sensitivity_utils import calculate_variance_reduction\n",
    "\n",
    "# Scenario 1: Control top parameter only\n",
    "top_param = significant_params[0] if significant_params else ranking_apogee[0][0]\n",
    "reduction_top = calculate_variance_reduction(\n",
    "    sensitivities,\n",
    "    [top_param]\n",
    ")\n",
    "\n",
    "print(\"Design Improvement Scenarios:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nScenario 1: Control {top_param}\")\n",
    "print(f\"  Variance reduction: {reduction_top:.1f}%\")\n",
    "print(f\"  Action: Improve measurement/manufacturing tolerance\")\n",
    "\n",
    "# Scenario 2: Control top 2 parameters\n",
    "if len(significant_params) >= 2:\n",
    "    top_2_params = significant_params[:2]\n",
    "    reduction_top2 = calculate_variance_reduction(sensitivities, top_2_params)\n",
    "    \n",
    "    print(f\"\\nScenario 2: Control {' and '.join(top_2_params)}\")\n",
    "    print(f\"  Variance reduction: {reduction_top2:.1f}%\")\n",
    "    print(f\"  Action: Focus quality control on these two parameters\")\n",
    "\n",
    "# Scenario 3: Control all significant parameters\n",
    "if significant_params:\n",
    "    reduction_all = calculate_variance_reduction(sensitivities, significant_params)\n",
    "    \n",
    "    print(f\"\\nScenario 3: Control all significant parameters\")\n",
    "    print(f\"  Parameters: {len(significant_params)}\")\n",
    "    print(f\"  Variance reduction: {reduction_all:.1f}%\")\n",
    "    print(f\"  Remaining uncertainty: {100 - reduction_all:.1f}% (from non-significant params + LAE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Recommendations\n",
    "\n",
    "Based on the sensitivity analysis:\n",
    "\n",
    "1. **Focus Quality Control**: Prioritize measurement/manufacturing tolerance for high-sensitivity parameters\n",
    "\n",
    "2. **Cost-Benefit Analysis**: \n",
    "   - Tightening tolerance on low-sensitivity parameters has minimal impact\n",
    "   - Investment should match sensitivity ranking\n",
    "\n",
    "3. **Flight Predictions**:\n",
    "   - Prediction uncertainty is dominated by high-sensitivity parameters\n",
    "   - Better measurements of these parameters will improve prediction accuracy\n",
    "\n",
    "4. **Design Margins**:\n",
    "   - Account for uncertainty propagation from significant parameters\n",
    "   - Use prediction intervals for safety margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Save Results for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Monte Carlo data in RocketPy format\n",
    "mc_output_dir = output_dir / 'monte_carlo_data'\n",
    "\n",
    "input_path, output_path = mc_runner.save_rocketpy_format(\n",
    "    output_dir=str(mc_output_dir),\n",
    "    filename_prefix=\"sensitivity_example\"\n",
    ")\n",
    "\n",
    "print(f\"Monte Carlo data saved:\")\n",
    "print(f\"  Input:  {input_path}\")\n",
    "print(f\"  Output: {output_path}\")\n",
    "\n",
    "# Export sensitivity results\n",
    "from sensitivity_utils import export_sensitivity_to_json\n",
    "\n",
    "json_path = output_dir / 'sensitivity_results.json'\n",
    "\n",
    "export_sensitivity_to_json(\n",
    "    sensitivity_coefficients=analyzer.sensitivity_coefficients,\n",
    "    lae=analyzer.lae,\n",
    "    nominal_parameters=metadata,\n",
    "    output_file=json_path\n",
    ")\n",
    "\n",
    "print(f\"\\nSensitivity results saved: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Variance-based sensitivity** provides quantitative variance contribution with statistical validation\n",
    "2. **OAT sensitivity** is useful for quick screening and understanding directional effects\n",
    "3. **LAE** validates the linear approximation assumption\n",
    "4. **Sensitivity ranking** guides design priorities and quality control focus\n",
    "\n",
    "### Recommended Workflow\n",
    "\n",
    "```\n",
    "1. Monte Carlo simulation (100+ samples)\n",
    "   ↓\n",
    "2. Variance-based sensitivity analysis\n",
    "   ↓\n",
    "3. Check LAE:\n",
    "   - LAE < 10%: Results are reliable ✓\n",
    "   - LAE > 30%: Consider non-linear effects or more samples\n",
    "   ↓\n",
    "4. Focus on parameters with sensitivity > LAE\n",
    "   ↓\n",
    "5. Design improvements based on variance reduction potential\n",
    "```\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- RocketPy Documentation: [Sensitivity Analysis](https://docs.rocketpy.org/)\n",
    "- Sobol' variance decomposition theory\n",
    "- Morris screening method for large parameter spaces\n",
    "\n",
    "### Try It Yourself\n",
    "\n",
    "1. Modify parameter uncertainties and re-run\n",
    "2. Add more parameters to the analysis\n",
    "3. Analyze different target variables (max velocity, landing distance)\n",
    "4. Compare results with different rocket configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}